{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8eb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c084cf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e31fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tensorflow 2.0 run eager execution \n",
    "\"\"\"\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# Y_hat = tf.constant(23, name = \"Y_hat\", shape = (1, ))\n",
    "# Y = tf.constant(32, name = \"Y\", shape = (1, ))\n",
    "# loss = tf.Variable((Y - Y_hat) ** 2, name = \"loss\")\n",
    "# init = tf.compat.v1.global_variables_initializer()\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     print(sess.run(loss))\n",
    "    \n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1d933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=23>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = tf.constant(23, name = \"Y_hat\")\n",
    "tf.keras.backend.clear_session()\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46875662",
   "metadata": {},
   "source": [
    "The key differences between `tf.Variable` and `tf.constant` in TensorFlow lie in their mutability and purpose:\n",
    "\n",
    "**1. Mutability:**\n",
    "\n",
    "- **tf.Variable:** **Mutable:** Its value can be changed after creation using methods like `assign` or through automatic updates during training. This makes them ideal for storing trainable parameters in neural networks.\n",
    "- **tf.constant:** **Immutable:** Once created, its value cannot be changed. It's suitable for storing fixed values that won't change during the execution of your program.\n",
    "\n",
    "**2. Purpose:**\n",
    "\n",
    "- **tf.Variable:** Primarily used for **trainable parameters** in machine learning models like weights and biases in neural networks. These parameters are updated during the training process to learn patterns from the data.\n",
    "- **tf.constant:** Used for **fixed values** that don't need to be modified during program execution. Examples include input data, hyperparameters, or constants like pi or the speed of light.\n",
    "\n",
    "**3. Behavior in the Graph:**\n",
    "\n",
    "- **tf.Variable:** Creates a **placeholder node** in the TensorFlow graph. This node holds a reference to the actual variable's value, which is stored separately. During training, updates to the variable are reflected in the graph through operations like `assign`.\n",
    "- **tf.constant:** Creates a **constant node** in the graph that holds the actual value itself. This node's value is fixed and cannot be changed during the execution of the graph.\n",
    "\n",
    "**4. Optimization:**\n",
    "\n",
    "- **tf.Variable:** Plays a crucial role in **backpropagation** during training. Gradients are calculated with respect to `tf.Variable`s to update the weights and biases in neural networks.\n",
    "- **tf.constant:** Does not participate in backpropagation since its value is fixed.\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Feature        | tf.Variable                         | tf.constant                       |\n",
    "|----------------|------------------------------------|-------------------------------------|\n",
    "| Mutability      | **Mutable** (can be changed)             | **Immutable** (cannot be changed)     |\n",
    "| Purpose         | Trainable parameters in models        | Fixed values in computations       |\n",
    "| Graph behavior   | Placeholder node (reference)         | Constant node (holds actual value) |\n",
    "| Optimization    | Used in backpropagation               | Not involved in backpropagation     |\n",
    "\n",
    "\n",
    "In your project, use `tf.Variable` judiciously for parameters requiring training and avoid unnecessary usage to optimize memory allocation and execution speed. Use `tf.constant` for values that remain fixed throughout your program's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd76986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'loss:0' shape=() dtype=int32, numpy=81>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"You can do directly\"\"\"\n",
    "Y_hat = tf.constant(23, name = \"Y_hat\")\n",
    "Y = tf.constant(32, name = \"Y\")\n",
    "loss = tf.Variable((Y - Y_hat) ** 2, name = \"loss\")\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debfc87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(23, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8173b3a",
   "metadata": {},
   "source": [
    "Writing and running programs in TensorFlow has the following steps:\n",
    "\n",
    "1. Create Tensors (variables) that are not yet executed/evaluated.\n",
    "2. Write operations between those Tensors.\n",
    "3. Initialize your Tensors.\n",
    "4. Create a Session.\n",
    "5. Run the Session. This will run the operations you'd written above.\n",
    "\n",
    "But tensorflow 2.0 supports egaer execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc032513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(20, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(10)\n",
    "c = tf.multiply(a, b)\n",
    "print(c)\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20e21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of x in the feed_dict\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# x = tf.compat.v1.placeholder(tf.int64, name = 'x')\n",
    "\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     print(sess.run(x * 2, feed_dict = {x : 2}))\n",
    "#     sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a5166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069   0.3190391 ]\n",
      " [-0.24937038  1.46210794 -2.06014071]], shape=(4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(np.random.randn(4, 3), name = \"X\")\n",
    "print(x)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b970b6",
   "metadata": {},
   "source": [
    "Lets start this programming exercise by computing the following equation:  ùëå=ùëäùëã+ùëè , where  ùëä  and  ùëã  are random matrices and b is a random vector.\n",
    "\n",
    "Exercise: Compute  ùëäùëã+ùëè  where  ùëä,ùëã , and  ùëè  are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, here is how you would define a constant X that has shape (3,1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cd0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1acbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function:\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    W = tf.constant(np.random.randn(4, 3))\n",
    "    X = tf.constant(np.random.randn(3, 1))\n",
    "    b = tf.constant(np.random.randn(4, 1))\n",
    "    Y = tf.add(tf.matmul(W, X), b)\n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddb0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = tf.Tensor(\n",
      "[[-1.10296141]\n",
      " [-1.03295748]\n",
      " [ 0.30515231]\n",
      " [-0.33862392]], shape=(4, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print( \"result = \" + str(linear_function()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88dcbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def Sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "\n",
    "    Returns:\n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.math.sigmoid(tf.constant(z, dtype = tf.float64, name = \"z\")).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f0a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) =  0.5\n",
      "sigmoid(12) = 0.9999938558253978\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \", Sigmoid(0))\n",
    "print (\"sigmoid(12) = \" + str(Sigmoid(12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f9d32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cost\n",
    "\n",
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "¬†¬†¬†¬†Computes the cost using the sigmoid cross entropy\n",
    "¬†¬†¬†¬†\n",
    "¬†¬†¬†¬†Arguments:\n",
    "¬†¬†¬†¬†logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "¬†¬†¬†¬†labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "¬†¬†¬†¬†\n",
    "¬†¬†¬†¬†Returns:\n",
    "¬†¬†¬†¬†cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits = tf.constant(logits, name = \"logits\", dtype = tf.float64), labels = tf.constant(labels, name = \"labels\", dtype = tf.float64)).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "319d6166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [1.00538722 1.03664083 0.41385432 0.39956614]\n"
     ]
    }
   ],
   "source": [
    "logits = Sigmoid(np.array([0.2,0.4,0.7,0.9]))\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c53562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                      \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    return tf.one_hot(tf.constant(labels, dtype = tf.int32), tf.constant(C, dtype = tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5db84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b445db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ones\n",
    "\n",
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.ones(tf.constant(shape, dtype = tf.int32)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d196c239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print (\"ones = \" + str(ones([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da57bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6febd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd6c1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (12288, 1080)\n",
      "Y_train shape: (6, 1080)\n",
      "X_test shape: (12288, 120)\n",
      "Y_test shape: (6, 120)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten/255.\n",
    "X_test = X_test_flatten/255.\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c87cb92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
       "array([[ 0.525599  ,  0.13712859,  0.5714648 ],\n",
       "       [ 0.68643475, -0.5642214 , -0.12864947],\n",
       "       [-0.53559923,  0.6122489 ,  0.10256004]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3,3)\n",
    "initializer = tf.initializers.GlorotUniform()\n",
    "var = tf.Variable(initializer(shape=shape))\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e5349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(1)\n",
    "    initializer = tf.initializers.GlorotNormal(seed=1)\n",
    "    W1 = tf.Variable(initializer(shape = (25,12288)), trainable = True, dtype=tf.float32)\n",
    "    b1 = tf.Variable(initializer(shape = (25,1)), trainable = True, dtype=tf.float32)\n",
    "    W2 = tf.Variable(initializer(shape = (12,25)), trainable = True, dtype=tf.float32)\n",
    "    b2 = tf.Variable(initializer(shape = (12,1)), trainable = True, dtype=tf.float32)\n",
    "    W3 = tf.Variable(initializer(shape = (6,12)), trainable = True, dtype=tf.float32)\n",
    "    b3 = tf.Variable(initializer(shape = (6,1)), trainable = True, dtype=tf.float32)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d030a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'Variable:0' shape=(25, 12288) dtype=float32, numpy=\n",
      "array([[ 0.00183502, -0.00714326,  0.00914525, ..., -0.01221176,\n",
      "         0.01656458,  0.00526992],\n",
      "       [ 0.02264025,  0.0067227 ,  0.00795862, ...,  0.00284724,\n",
      "         0.01910819,  0.00122853],\n",
      "       [-0.00163333, -0.00861273, -0.01398893, ..., -0.00741132,\n",
      "         0.02071251, -0.026417  ],\n",
      "       ...,\n",
      "       [-0.00126929,  0.01729332,  0.02082342, ...,  0.01709594,\n",
      "         0.00429358, -0.00733263],\n",
      "       [ 0.00275988,  0.00419503,  0.00949959, ...,  0.01241149,\n",
      "        -0.0270652 ,  0.01501104],\n",
      "       [-0.00145541,  0.02459595,  0.00339064, ..., -0.02478788,\n",
      "         0.02716016, -0.00306428]], dtype=float32)>\n",
      "b1 = <tf.Variable 'Variable:0' shape=(25, 1) dtype=float32, numpy=\n",
      "array([[ 0.03993344],\n",
      "       [-0.15545043],\n",
      "       [ 0.19901739],\n",
      "       [-0.24872702],\n",
      "       [-0.28688198],\n",
      "       [-0.12585859],\n",
      "       [-0.01201915],\n",
      "       [ 0.14756493],\n",
      "       [-0.00472601],\n",
      "       [-0.44236282],\n",
      "       [ 0.54058254],\n",
      "       [-0.4345032 ],\n",
      "       [ 0.11800031],\n",
      "       [ 0.21523887],\n",
      "       [-0.06789926],\n",
      "       [-0.16455258],\n",
      "       [-0.05258802],\n",
      "       [-0.18575548],\n",
      "       [-0.00277632],\n",
      "       [-0.06777475],\n",
      "       [ 0.09226809],\n",
      "       [ 0.02067652],\n",
      "       [-0.05682073],\n",
      "       [ 0.37068033],\n",
      "       [ 0.21586621]], dtype=float32)>\n",
      "W2 = <tf.Variable 'Variable:0' shape=(12, 25) dtype=float32, numpy=\n",
      "array([[ 0.03347515, -0.13031   ,  0.16683103, -0.20850131, -0.2404856 ,\n",
      "        -0.10550394, -0.01007534,  0.12369979, -0.00396169, -0.3708211 ,\n",
      "         0.4531561 , -0.36423257,  0.09891655,  0.18042907, -0.05691816,\n",
      "        -0.13794008, -0.04408315, -0.15571392, -0.00232732, -0.05681378,\n",
      "         0.07734591,  0.01733258, -0.04763132,  0.31073156,  0.18095495],\n",
      "       [ 0.27575395,  0.0652916 ,  0.1945817 ,  0.00809532, -0.35056123,\n",
      "        -0.04349809,  0.00533627,  0.14254977, -0.22560681, -0.07908897,\n",
      "        -0.10430467, -0.27877635, -0.22633284, -0.15467171, -0.30278656,\n",
      "         0.42898563,  0.04269395,  0.14339304,  0.4080808 ,  0.19127996,\n",
      "        -0.08289494,  0.19833343, -0.18854785,  0.11080088, -0.10293514],\n",
      "       [ 0.07490641,  0.12879197, -0.37855393, -0.14283597, -0.1686482 ,\n",
      "        -0.12487295,  0.08058943, -0.14054622, -0.32778224,  0.11833672,\n",
      "         0.21914457,  0.1428981 ,  0.4125568 , -0.02966296, -0.11308558,\n",
      "         0.2839564 ,  0.26005846, -0.12101684,  0.14712207, -0.39992067,\n",
      "        -0.11544652, -0.11918075, -0.5031594 , -0.16591036, -0.0463655 ],\n",
      "       [-0.11970381,  0.19529893, -0.13428073, -0.46205577,  0.07818031,\n",
      "        -0.37026545, -0.06378475,  0.3693177 ,  0.07429875,  0.5162154 ,\n",
      "        -0.01673712,  0.35476214,  0.09180938,  0.17954443,  0.00366851,\n",
      "         0.04273273,  0.10226952,  0.03479045, -0.23308858, -0.23678231,\n",
      "        -0.07444265, -0.30713868, -0.11694647,  0.32906806, -0.09511968],\n",
      "       [ 0.1597296 ,  0.0393942 ,  0.4806439 ,  0.22657786,  0.03705936,\n",
      "        -0.51946974, -0.01744555, -0.3121316 , -0.21262619,  0.05213222,\n",
      "         0.04905998, -0.29094276, -0.03067003,  0.47902155,  0.3192982 ,\n",
      "         0.04899108,  0.08141489,  0.32032356, -0.02520937,  0.10048122,\n",
      "        -0.05332499, -0.34107792, -0.13928485,  0.12382392, -0.41300818],\n",
      "       [-0.15009922,  0.03965309, -0.47955167, -0.07975382,  0.09735033,\n",
      "        -0.00232943, -0.26368296, -0.24157847,  0.24642426,  0.22330649,\n",
      "        -0.30809   ,  0.1017215 ,  0.02995418,  0.26468748, -0.52045006,\n",
      "        -0.08780679,  0.02792993,  0.2803117 ,  0.20731722, -0.14461054,\n",
      "        -0.09631125,  0.2553377 ,  0.0313108 ,  0.28631765,  0.02228327],\n",
      "       [-0.20281379, -0.2922766 , -0.02937314,  0.00603206,  0.34428513,\n",
      "         0.14963914, -0.42720488,  0.07914864,  0.06170019, -0.1942032 ,\n",
      "         0.03054667, -0.20949648,  0.28976992,  0.03168807,  0.18336946,\n",
      "        -0.1762811 , -0.21549895,  0.02550345, -0.17974655,  0.20999093,\n",
      "         0.13074148,  0.12900151, -0.29620144,  0.39836082,  0.35581756],\n",
      "       [-0.0809828 ,  0.0508789 ,  0.04040742, -0.06884032, -0.07758227,\n",
      "         0.21241859,  0.16171476, -0.0574309 , -0.04828556, -0.23045349,\n",
      "         0.25515756, -0.2933403 , -0.16053016, -0.11232601, -0.13022825,\n",
      "         0.05022161,  0.18676145, -0.0776429 ,  0.1028303 , -0.06372993,\n",
      "         0.41251048, -0.01803587,  0.04746069,  0.27630135, -0.21901166],\n",
      "       [ 0.28674722,  0.20629272, -0.38042602,  0.2629734 ,  0.23505901,\n",
      "         0.18131882,  0.25507575, -0.18465258,  0.36351705,  0.02036598,\n",
      "        -0.33226338, -0.09722907, -0.00817799,  0.22310142, -0.22345006,\n",
      "        -0.02439135,  0.19453065,  0.00433169,  0.47482875,  0.00131025,\n",
      "         0.3148377 , -0.22662118,  0.12927507,  0.04369456, -0.45121887],\n",
      "       [-0.23054187, -0.22334962, -0.18913192,  0.15417175, -0.07368277,\n",
      "        -0.0554374 ,  0.12214173,  0.3880139 , -0.01242276,  0.11768965,\n",
      "         0.26777858, -0.06251994, -0.12100054, -0.12495217, -0.03189994,\n",
      "        -0.50085783, -0.09560107, -0.2402923 ,  0.07087833,  0.03642716,\n",
      "        -0.00494978, -0.36984688,  0.00878784,  0.24595837, -0.1323934 ],\n",
      "       [ 0.318445  ,  0.02266271, -0.06839968, -0.33999097,  0.36428225,\n",
      "        -0.29877323, -0.05122564, -0.37625378,  0.268155  ,  0.19277745,\n",
      "         0.0209468 ,  0.10230298, -0.3974174 ,  0.02363082,  0.12647092,\n",
      "         0.33263046,  0.07712924,  0.19261338, -0.08101249, -0.28784147,\n",
      "         0.17327178, -0.1326688 ,  0.28894275, -0.1987789 , -0.03405774],\n",
      "       [ 0.1867647 , -0.20398362, -0.03936024, -0.3679167 , -0.22960074,\n",
      "         0.23853372, -0.04239364, -0.02102038, -0.06170619,  0.15821125,\n",
      "        -0.31986177,  0.15379827,  0.14520119, -0.24647985, -0.09176766,\n",
      "         0.13322504,  0.40508434,  0.35624555,  0.3348011 ,  0.05405051,\n",
      "         0.21186371,  0.0197499 ,  0.45979315,  0.04328793,  0.36662805]],\n",
      "      dtype=float32)>\n",
      "b2 = <tf.Variable 'Variable:0' shape=(12, 1) dtype=float32, numpy=\n",
      "array([[ 0.0564744 ],\n",
      "       [-0.21984011],\n",
      "       [ 0.2814531 ],\n",
      "       [-0.35175312],\n",
      "       [-0.40571237],\n",
      "       [-0.17799091],\n",
      "       [-0.01699764],\n",
      "       [ 0.20868832],\n",
      "       [-0.00668358],\n",
      "       [-0.6255955 ],\n",
      "       [ 0.7644991 ],\n",
      "       [-0.61448026]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11916a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)                        # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)                       # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,Z2),b3)                       # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05a77125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = tf.Tensor(\n",
      "[[ 0.31374222  0.7606991   0.38721418 ...  0.48126855  0.5716481\n",
      "   0.5039601 ]\n",
      " [ 0.45215803  0.09779096  0.26445144 ...  0.26757234  0.30503505\n",
      "   0.3762372 ]\n",
      " [ 1.6638794   1.3688543   1.4704036  ...  1.4247197   1.3327775\n",
      "   1.7511069 ]\n",
      " [-1.2653809  -0.8749666  -0.94326675 ... -1.0216074  -1.1434367\n",
      "  -0.9625934 ]\n",
      " [ 0.5738419   0.24137753  0.40707517 ...  0.45794368  0.5317998\n",
      "   0.41900015]\n",
      " [ 1.3672092   1.1985708   1.2424432  ...  1.4217482   1.3011042\n",
      "   0.9124247 ]], shape=(6, 1080), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X_train, parameters)\n",
    "print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6021bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acbb9c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = tf.Tensor(2.0450943, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X_train, parameters)\n",
    "cost = compute_cost(Z3, Y_train)\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4352411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    x = tf.constant(3.0)\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4e162",
   "metadata": {},
   "source": [
    "## Understanding `tf.GradientTape()` and its Use in Training Loops:\n",
    "\n",
    "**1. Automatic Differentiation with `tf.GradientTape()`:**\n",
    "\n",
    "- `tf.GradientTape()` is a powerful tool in TensorFlow for **automatic differentiation**. This involves efficiently computing the gradients of a computation with respect to its inputs.\n",
    "\n",
    "**2. Working with `tf.GradientTape()**:\n",
    "\n",
    "- **Context Manager:** It acts as a context manager, meaning you use it within a `with` block. Any operations performed inside the `with` block are recorded by the tape.\n",
    "- **Recording Operations:** The tape tracks all mathematical operations performed on tensors within the `with` block. This includes functions like addition, multiplication, and activation functions.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x = tf.constant([1.0, 2.0])\n",
    "  y = x * x  # Operation recorded by the tape\n",
    "\n",
    "# After the 'with' block, the tape contains the computation history\n",
    "```\n",
    "\n",
    "**3. Retrieving Gradients:**\n",
    "\n",
    "- Once you've built the computational graph within the `with` block, you can use the `tape.gradient` method to calculate the gradients of any recorded output with respect to any input variable.\n",
    "\n",
    "```python\n",
    "# Calculate the gradient of y with respect to x\n",
    "gradients = tape.gradient(y, x)\n",
    "print(gradients)  # Output: [2. 4.]\n",
    "```\n",
    "\n",
    "**4. Application in Training Loops:**\n",
    "\n",
    "**a. Forward Pass:**\n",
    "\n",
    "- During training, the forward pass involves calculating the model's output (predictions) for a given input (features). You can use `tf.GradientTape()` to record the operations performed in the forward pass.\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass operations using your model and input data\n",
    "  predictions = model(data)\n",
    "  loss = loss_function(predictions, labels)\n",
    "```\n",
    "\n",
    "**b. Backward Pass (Gradient Calculation):**\n",
    "\n",
    "- After the forward pass, use the tape's `gradient` method to calculate the gradients of the loss function with respect to the trainable variables of the model (weights and biases). These gradients are crucial for updating the model's parameters in the direction that minimizes the loss.\n",
    "\n",
    "```python\n",
    "# Calculate gradients of loss w.r.t. trainable variables\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "```\n",
    "\n",
    "**c. Optimizer Update:**\n",
    "\n",
    "- You can use an optimizer (e.g., `tf.keras.optimizers.Adam`) to update the model's trainable variables based on the calculated gradients. This update aims to improve the model's performance by reducing the loss over multiple training iterations.\n",
    "\n",
    "```python\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```\n",
    "\n",
    "This process of forward pass, gradient calculation, and parameter update forms the core of backpropagation, which is the foundation of training deep neural networks.\n",
    "\n",
    "**Overall, `tf.GradientTape()` simplifies automatic differentiation within TensorFlow, making it easier to build and train models by automatically tracking operations and calculating gradients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffbb6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 2500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.random.set_seed(1)                            # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    #print(parameters)\n",
    "    \n",
    "    optimizer = tf.optimizers.SGD(learning_rate=learning_rate, name='SGD')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                ### In GradientTape we have to pass all the Operations and tape will records all those things\n",
    "                Z3 = forward_propagation(minibatch_X, parameters)\n",
    "                loss_value = compute_cost(Z3, minibatch_Y)\n",
    "              \n",
    "            # contains values of gradients of models weights like\n",
    "            # {W1 : [W1 gradients values]}\n",
    "            grads = tape.gradient(loss_value, [parameters])\n",
    "            processed_grads = [value for key, value in grads[0].items()]\n",
    "            processed_parameters = [value for key, value in parameters.items()]\n",
    "            \n",
    "            # We have to pass list of grads values along with parameters both are in list format\n",
    "            optimizer.apply_gradients(zip(processed_grads, processed_parameters))\n",
    "            epoch_cost += loss_value\n",
    "        \n",
    "        # Print the cost every epoch\n",
    "        if print_cost == True and epoch % 100 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    #lets save the parameters in a variable\n",
    "#     parameters = sess.run(parameters)\n",
    "#     print (\"Parameters have been trained!\")\n",
    "\n",
    "    #Calculate the correct predictions\n",
    "    #correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "598824db",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 65.573296\n",
      "Cost after epoch 100: 50.508659\n",
      "Cost after epoch 200: 43.745728\n",
      "Cost after epoch 300: 38.652092\n",
      "Cost after epoch 400: 34.612400\n",
      "Cost after epoch 500: 31.407511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[0;32m     45\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m compute_cost(Z3, minibatch_Y)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# contains values of gradients of models weights like\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# {W1 : [W1 gradients values]}\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m processed_grads \u001b[38;5;241m=\u001b[39m [value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m grads[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     51\u001b[0m processed_parameters \u001b[38;5;241m=\u001b[39m [value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mitems()]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1058\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1059\u001b[0m           output_gradients))\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1063\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1072\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:535\u001b[0m, in \u001b[0;36m_SoftmaxCrossEntropyWithLogitsGrad\u001b[1;34m(op, grad_loss, grad_grad)\u001b[0m\n\u001b[0;32m    527\u001b[0m   softmax \u001b[38;5;241m=\u001b[39m nn_ops\u001b[38;5;241m.\u001b[39msoftmax(logits)\n\u001b[0;32m    529\u001b[0m   grad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((grad_grad \u001b[38;5;241m-\u001b[39m array_ops\u001b[38;5;241m.\u001b[39msqueeze(\n\u001b[0;32m    530\u001b[0m       math_ops\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[0;32m    531\u001b[0m           array_ops\u001b[38;5;241m.\u001b[39mexpand_dims(grad_grad, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    532\u001b[0m           array_ops\u001b[38;5;241m.\u001b[39mexpand_dims(softmax, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    533\u001b[0m       axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m softmax)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad, _BroadcastMul(grad_loss, \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7498\u001b[0m, in \u001b[0;36mneg\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m   7496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   7497\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 7498\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7499\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   7501\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    W1 = tf.Variable(initializer(shape = (25,12288)), trainable = True, dtype=tf.float32)\n",
    "    b1 = tf.Variable(initializer(shape = (25,1)), trainable = True, dtype=tf.float32)\n",
    "    W2 = tf.Variable(initializer(shape = (12,25)), trainable = True, dtype=tf.float32)\n",
    "    b2 = tf.Variable(initializer(shape = (12,1)), trainable = True, dtype=tf.float32)\n",
    "    W3 = tf.Variable(initializer(shape = (6,12)), trainable = True, dtype=tf.float32)\n",
    "    b3 = tf.Variable(initializer(shape = (6,1)), trainable = True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494818ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(12288,), name=\"digits\")\n",
    "x1 = layers.Dense(25, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(12, activation=\"relu\")(x1)\n",
    "x3 = layers.Dense(6, activation=\"relu\")(x2)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x3)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753192bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422825c",
   "metadata": {},
   "source": [
    "####\n",
    "https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc58f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
